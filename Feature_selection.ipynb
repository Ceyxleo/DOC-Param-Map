{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Open Rituals**\n",
    "\n",
    "Import needed package and define paths and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% open rituals ##############################################################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "from pandas import MultiIndex, Int64Index\n",
    "from optuna.samplers import TPESampler\n",
    "import matplotlib.pyplot as plt\n",
    "from hyperopt import Trials\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "import optuna\n",
    "import spotpy\n",
    "import shap\n",
    "import math\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "\n",
    "### replace this main directory with your own\n",
    "Path_Main = r'C:\\Users\\lli55\\Desktop\\Lingbo Li PhD\\DOC project\\Model_with_SoilGrid'\n",
    "Path_Plot = os.path.join(Path_Main, 'plot')\n",
    "Path_Output = os.path.join(Path_Main, 'output')\n",
    "Path_Shape = os.path.join(Path_Main, 'shape')\n",
    "Path_Input = os.path.join(Path_Main, 'input')\n",
    "Path_Input_old = r'C:\\Users\\lli55\\Desktop\\Lingbo Li PhD\\DOC project\\Data'\n",
    "Path_Shape_old = r'C:\\Users\\lli55\\Desktop\\Lingbo Li PhD\\DOC project\\shape'\n",
    "Path_Soilgrid = r'C:\\Users\\lli55\\Desktop\\Lingbo Li PhD\\SoilGrid\\data'\n",
    "\n",
    "### KGE for model training\n",
    "def kge_2009(preds, Dtrain):\n",
    "    y = Dtrain.get_label()\n",
    "    kge = spotpy.objectivefunctions.kge(y, preds)\n",
    "    if math.isnan(kge):\n",
    "        kge_1 = -9999\n",
    "    else:\n",
    "        kge_1 = kge\n",
    "    return 'kge', kge_1\n",
    "\n",
    "## KGE for plotting\n",
    "def kge_2009_p(preds, obs):\n",
    "    return spotpy.objectivefunctions.kge(obs, preds)\n",
    "\n",
    "## func for CDF plot in representative checking\n",
    "def my_range(start, end, how_many):\n",
    "    incr = float(end - start)/(how_many-1)\n",
    "    return [start + i*incr for i in range(how_many)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Variable Selection**\n",
    "Here the feature selection is based on the pr calculated using SOC from SoilGrid2.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step one: read and process the data #######################################\n",
    "data = pd.read_csv(os.path.join(Path_Input, 'train_set.txt'), sep= '\\t')\n",
    "## Duo to the missing SOC data in SoilGrid2.0, we can not get valid value for 12 previous training stations \n",
    "data = data.dropna(subset = 'pr_soilgrid').reset_index(drop = True)\n",
    "feature = data.columns[:-3]\n",
    "## log transformation to the target value pr. Here pr calcuated using SOC from soilgrid is used\n",
    "X, Y = data[feature].copy(), np.log10(data['pr_soilgrid'])\n",
    "\n",
    "## introducing a randomly generated attributes into model\n",
    "np.random.seed(0)\n",
    "random = np.random.rand(len(X),1) \n",
    "power = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "trans_random = power.fit_transform(random)\n",
    "X['rand'] = trans_random\n",
    "Xx = X.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### repeat step two to six till no feature can be dropped #####################\n",
    "### step two: define the objective function ###################################\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(Xx, Y, test_size=0.3, random_state=1)\n",
    "Dtrain = xgb.DMatrix(X_train, label = Y_train)\n",
    "Dtest = xgb.DMatrix(X_test, label = Y_test)\n",
    "\n",
    "### define the objective funtion for optuna ###################################\n",
    "def objective_xgb(trial):     \n",
    "    param = {\n",
    "        'booster':'gbtree',\n",
    "        'lambda': trial.suggest_loguniform('lambda', 1e-2, 10), # default value = 1\n",
    "        'alpha': trial.suggest_loguniform('alpha', 1e-2, 10), # default value = 0\n",
    "        'gamma': trial.suggest_loguniform('gamma', 1e-2, 10), # default value = 0\n",
    "        'eta': trial.suggest_loguniform('eta', 1e-4, 1.0), # default value = 0.3\n",
    "        'max_delta_step': trial.suggest_loguniform('max_delta_step', 1, 10), # default value = 0\n",
    "        'min_child_weight': trial.suggest_loguniform('min_child_weight', 1e-1, 10), # default value = 1\n",
    "        'colsample_bytree': trial.suggest_loguniform('colsample_bytree', 5e-1, 1), # default value = 1\n",
    "        'subsample': trial.suggest_loguniform('subsample', 5e-1, 1), # default value = 1\n",
    "        'max_depth': trial.suggest_int('max_depth', 2, 10), # default value = 6  \n",
    "        'disable_default_eval_metric':1\n",
    "    }\n",
    "    ## put this into callback if you want to monitor the details\n",
    "    # xgb.callback.EvaluationMonitor(show_stdv=False)\n",
    "    xgb_cv_results = xgb.cv(param,\n",
    "                    Dtrain,\n",
    "                    num_boost_round=500,\n",
    "                    seed=42,\n",
    "                    nfold=5,\n",
    "                    maximize = True, \n",
    "                    feval = kge_2009,   \n",
    "                    callbacks=[\n",
    "                        # xgb.callback.EvaluationMonitor(show_stdv=False),\n",
    "                        xgb.callback.EarlyStopping(rounds = 50,\n",
    "                                                          metric_name = 'kge',\n",
    "                                                          maximize = True)],   \n",
    "                    verbose_eval=False)     \n",
    "                      \n",
    "    kge_ = xgb_cv_results.iloc[-1]['test-kge-mean'] \n",
    "    trial.set_user_attr('n_estimators', len(xgb_cv_results))                      \n",
    "    return kge_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## step three:optuna learning process #########################################\n",
    "optuna.logging.set_verbosity(optuna.logging.WARNING)\n",
    "study = optuna.create_study(direction='maximize',sampler=TPESampler(seed=42))\n",
    "study.optimize(objective_xgb, n_trials=200, show_progress_bar=True)\n",
    "print('Number of finished trials:', len(study.trials))\n",
    "best_param = study.best_trial.params\n",
    "best_param['booster'] = 'gbtree'\n",
    "best_param['disable_default_eval_metric'] = 1\n",
    "n_estimators = study.best_trial.user_attrs['n_estimators']\n",
    "early_stop = int(n_estimators/10)\n",
    "\n",
    "## CV to get the rigorois result of model preformance #########################\n",
    "optimised_xgb = xgb.train(\n",
    "                          best_param, \n",
    "                          Dtrain, \n",
    "                          num_boost_round=n_estimators, \n",
    "                          evals = [(Dtrain, 'eval_train'), (Dtest, 'eval_test')],\n",
    "                          feval=kge_2009,\n",
    "                          maximize = True,        \n",
    "                          callbacks=[xgb.callback.EvaluationMonitor(show_stdv=False),\n",
    "                                     xgb.callback.EarlyStopping(rounds = early_stop,\n",
    "                                                                metric_name = 'kge',\n",
    "                                                                maximize = True)],   \n",
    "                          verbose_eval=False\n",
    "                          ) \n",
    "\n",
    "predicted_mean_train = optimised_xgb.predict(Dtrain, iteration_range=(0, optimised_xgb.best_iteration+1))\n",
    "predicted_mean_test  = optimised_xgb.predict(Dtest, iteration_range=(0, optimised_xgb.best_iteration+1))\n",
    "\n",
    "## train/test scores of pr in log scale\n",
    "print('trainning_score:' + str(kge_2009(predicted_mean_train, Dtrain)))\n",
    "print('testing_score:' + str(kge_2009(predicted_mean_test, Dtest)))\n",
    "## train/test scores of pr in original scale\n",
    "print('trainning_score_normal:', kge_2009_p([10**i for i in predicted_mean_train], [10**i for i in Y_train]))\n",
    "print('testing_score_normal:' + str(kge_2009_p([10**i for i in predicted_mean_test], [10**i for i in Y_test])))\n",
    "print(best_param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 5 visulization: performance at testing ###############################\n",
    "path_feature_selection = os.path.join(Path_Plot, 'feature_selection')\n",
    "name = '8th'\n",
    "obs = [10**i for i in Y_test]\n",
    "sim = [10**i for i in predicted_mean_test]\n",
    "x = np.arange(0, 1e-2, 1e-4)\n",
    "fig = plt.figure(1, (7,7))\n",
    "ax = plt.gca()\n",
    "ax.scatter(obs, sim, label = \"Sim Vs Obs\", ec = 'k', fc = 'steelblue', linewidth = 0.5, alpha = 0.75)\n",
    "ax.plot(x,x, label = \"1:1 line\", c = 'r')\n",
    "ax.legend(loc = 'best')\n",
    "ax.set_xscale('log')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xlim(1e-5, 1e-2)\n",
    "ax.set_ylim(1e-5, 1e-2)\n",
    "ax.set_xlabel('Observation')\n",
    "ax.set_ylabel('Simulation')\n",
    "ax.text(3e-3, 1.3e-5,'KGE:' + str(\"{:.3f}\".format(kge_2009(predicted_mean_test, Dtest)[1])), style='italic')\n",
    "ax.text(3e-3, 2e-5,'KGE_N:' + str(\"{:.3f}\".format(kge_2009_p(sim, obs))), style='italic')\n",
    "ax.set_title(name + '_run_performance')\n",
    "fig.savefig(os.path.join(path_feature_selection, name + '_run_performance.png'), dpi = 400)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 5 visulization: feature importance (only those important than rand)###\n",
    "explainer = shap.TreeExplainer(optimised_xgb)\n",
    "shap_values = explainer.shap_values(Xx)\n",
    "shap_sum = np.abs(shap_values).mean(axis=0)\n",
    "## It's recommanded to save the feature importance of each iterations\n",
    "importance_df = pd.DataFrame([Xx.columns.tolist(), shap_sum.tolist()]).T\n",
    "importance_df.columns = ['column_name', 'shap_importance']\n",
    "importance_df = importance_df.sort_values('shap_importance', ascending=False).reset_index(drop = True)\n",
    "importance_rand = importance_df[importance_df['column_name'] == 'rand']['shap_importance'].values[0]\n",
    "keep = [ i for i in importance_df[importance_df['shap_importance']>= importance_rand]['column_name']]\n",
    "shap.summary_plot(shap_values, Xx, plot_type='bar',max_display=len(keep),show = False)\n",
    "plt.savefig(os.path.join(path_feature_selection, name + '_run_improtance.png'),dpi = 400, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### step 6: only preserve features with importance higher than rand ###########\n",
    "Xx = Xx[keep]\n",
    "print(keep)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Representative check**\n",
    "- Feature selection yeilds the following important features: ['TOT_A', 'TOT_NLCD01_90', 'TOT_CONTACT', 'TOT_B', 'TOT_I', 'TOT_BFI', 'TOT_E', 'TOT_CLAYAVE', 'TOT_HGB', 'TOT_NLCD01_42', 'TOT_NLCD01_95', 'TOT_CNPY11_BUFF100', 'TOT_HGBD']\n",
    "- We need to further check if those predictors at training catchment are representative enough in CONUS domain\n",
    "- Representative is done through comparing the distribution and percentiles of those predictors in conus and training catchment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% representative checking ###################################################\n",
    "## read in the preprocessed predictors for training, evaluation, and prediction \n",
    "all_data = pd.read_csv(os.path.join(Path_Input, 'original_attributes.txt'), delimiter= '\\t').replace(-9999.0, np.nan) \n",
    "## corr.pkl contains the information of 9 correlation groups\n",
    "with open(os.path.join(Path_Input, 'corr.pkl'), 'rb') as f:\n",
    "    corr = pickle.load(f)\n",
    "\n",
    "selected_dependent_columns = ['TOT_NLCD01_90', 'TOT_CONTACT', 'TOT_BFI', 'TOT_CLAYAVE', 'TOT_HGB', 'TOT_NLCD01_42', 'TOT_NLCD01_95', 'TOT_CNPY11_BUFF100', 'TOT_HGBD']\n",
    "\n",
    "all_data_r = all_data[selected_dependent_columns]\n",
    "all_data_r['TOT_A'] = all_data[corr[1][0]].mean(axis=1, skipna=False)\n",
    "all_data_r['TOT_B'] = all_data[corr[1][1]].mean(axis=1, skipna=False)\n",
    "all_data_r['TOT_I'] = all_data[corr[1][9]].mean(axis=1, skipna=False)\n",
    "all_data_r['TOT_E'] = all_data[corr[1][5]].mean(axis=1, skipna=False)\n",
    "all_data_r['label'] = all_data['label']\n",
    "all_data_r['COMID'] = all_data['COMID']\n",
    "\n",
    "rep_conus = all_data_r[all_data_r['label'] == 'predict'].reset_index(drop = True)\n",
    "rep_conus.to_csv(os.path.join(Path_Output, 'rep_conus.txt'), sep = '\\t', index = None)\n",
    "\n",
    "rep_select = all_data_r[all_data_r['label'] == 'train'].reset_index(drop = True)\n",
    "''' \n",
    "Out of the previous 2595 training catchments (using HWSD), 12 catchments could not yield a valid PR due to missing values in the SoilGrid SOC data. As a result, we have to exclude these sites from the representativeness check. The following four lines are not necessary for the model using HWSD.\n",
    "'''\n",
    "pr_selected = pd.read_csv(os.path.join(Path_Input, 'train_set.txt'), sep= '\\t')[['COMID', 'pr_hwsd', 'pr_soilgrid']]\n",
    "rep_select = rep_select.merge(pr_selected, on = 'COMID', how = 'left')\n",
    "rep_select = rep_select.dropna().reset_index(drop = True)\n",
    "rep_select = rep_select.drop(columns=['pr_hwsd','pr_soilgrid'])\n",
    "rep_select.to_csv(os.path.join(Path_Output, 'rep_select.txt'), sep = '\\t', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% representative plot \n",
    "attr_sites = pd.read_csv(os.path.join(Path_Output,'rep_select.txt'), delimiter= '\\t')\n",
    "attr_flowlines = pd.read_csv(os.path.join(Path_Output, 'rep_conus.txt'), delimiter= '\\t').replace(np.nan, -9999.0)\n",
    "columns_tot =  ['TOT_A', 'TOT_NLCD01_90', 'TOT_CONTACT', 'TOT_B', 'TOT_I', 'TOT_BFI', 'TOT_E', 'TOT_CLAYAVE', 'TOT_HGB', 'TOT_NLCD01_42', 'TOT_NLCD01_95', 'TOT_CNPY11_BUFF100', 'TOT_HGBD']\n",
    "\n",
    "title = ['hydro_related', 'NLCD01_90', 'CONTACT' , 'temp_related', 'elev_related', 'BFI', 'soil_texture_related', 'CLAYAVE', 'HGB', 'NLCD01_42', 'NLCD01_95', 'CNPY11_BUFF100', 'HGBD']\n",
    "\n",
    "fig, axs = plt.subplots(4,4, figsize=(15, 15), facecolor='w', edgecolor='k')\n",
    "fig.subplots_adjust(hspace = 0.3, wspace=0.15)\n",
    "quantile = ['$p-5$','$p-25$','$p-50$','$p-75$','$p-95$']\n",
    "quantiles = [0.05,0.25,0.5,0.75,0.95]\n",
    "log_list = [0, 1, 4, 13]\n",
    "marker = ['.','o','v','s','*']\n",
    "color = ['y','g','r','c','k']\n",
    "quantile_ndhp = np.zeros((13,5))\n",
    "quantile_select = np.zeros((13,5))\n",
    "axs = axs.ravel()\n",
    "\n",
    "for i in range(13):\n",
    "    x_all_flowline_1 = sorted([i for i in attr_flowlines[columns_tot[i]] if i != -9999.0])\n",
    "    ## zero values are excluded in representative checking as XGBoost have self-aware for sparse data, which can handle both missing and larger portion of zero values\n",
    "    x_all_flowline = [i for i in x_all_flowline_1 if i != 0]\n",
    "    y_all_flowline = my_range(0,1,len(x_all_flowline))\n",
    "    \n",
    "    x_selected_site_1 = attr_sites.sort_values(by=[columns_tot[i]])[columns_tot[i]]\n",
    "    x_selected_site = [i for i in x_selected_site_1 if i not in [0]]\n",
    "    y_selected_site = my_range(0,1,len(x_selected_site))\n",
    "    axs[i].plot(x_all_flowline, y_all_flowline, 'b--', label = 'NHD+')\n",
    "    axs[i].plot(x_selected_site, y_selected_site, '-', color = 'orange', label = 'Training Data')\n",
    "    q_nhd = np.quantile(x_all_flowline, [0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    quantile_ndhp[i] = q_nhd\n",
    "    q_selected = np.quantile(x_selected_site, [0.05, 0.25, 0.5, 0.75, 0.95])\n",
    "    quantile_select[i] = q_selected\n",
    "    for j in range(5):\n",
    "        if q_nhd[j] != 0:\n",
    "            axs[i].plot(q_nhd[j],quantiles[j],marker[j]+color[j],label = quantile[j])\n",
    "    for k in range(5):        \n",
    "        if q_selected[k] != 0:\n",
    "            axs[i].plot(q_selected[k],quantiles[k],marker[k]+color[k])\n",
    "    axs[i].set_title(title[i])\n",
    "    axs[i].title.set_fontsize(18)\n",
    "    axs[i].yaxis.label.set_fontsize(16)\n",
    "    axs[i].tick_params(axis='x', labelsize=16) \n",
    "    axs[i].tick_params(axis='y', labelsize=16)\n",
    "    if i in log_list:\n",
    "        axs[i].set_xscale('log')\n",
    "    if i not in [0, 4, 8, 12]:\n",
    "        axs[i].set_yticks([])\n",
    "\n",
    "lines, labels = fig.axes[1].get_legend_handles_labels() \n",
    "axs[15].legend(lines, labels, loc='center left', prop = {'size' : 18})\n",
    "axs[15].axis('off')\n",
    "axs[14].axis('off')\n",
    "axs[13].axis('off')\n",
    "axs[4].set_xlim(left=0.1)\n",
    "fig.savefig(os.path.join(Path_Plot,'representative.png'),dpi = 600, bbox_inches='tight')\n",
    "## save this ratio information\n",
    "ratio = 2*np.abs(quantile_ndhp - quantile_select)/(quantile_ndhp + quantile_select)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f83720f3524ad70a07f8b9522dddf168ead5e164ced993ad70b21c480e663491"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('Hyriver': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
