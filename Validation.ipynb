{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Open Rituals**\n",
    "\n",
    "Import needed package and define paths and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%% open rituals ##############################################################\n",
    "import os \n",
    "import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "## define paths and functions ################################################\n",
    "### replace this main directory with your own\n",
    "Path_Main = r'C:\\Users\\lli55\\Desktop\\Lingbo Li PhD\\DOC project\\Model_with_SoilGrid'\n",
    "Path_Plot = os.path.join(Path_Main, 'plot')\n",
    "Path_Output = os.path.join(Path_Main, 'output')\n",
    "Path_Shape = os.path.join(Path_Main, 'shape')\n",
    "Path_Input = os.path.join(Path_Main, 'input')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Back-Calculation of DOC and $P_r$ at Evaluation Catchments**\n",
    "- a) For each NHDPlus local catchment, the long-time averaged DOC concentration can be calculated using SOC time model predicted $P_r$\n",
    "- b) Each train or eval catchments are composed for several NHDPlus local catchment. Idealy, the area-weighted average Pr and DOC should be close to the real measurement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step1: calculate the derived doc at each NHDPlus local catchments**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load geospatial and predicted Pr over 2.6 million local catchment\n",
    "data = gpd.read_file(os.path.join(Path_Shape, 'SOC.gpkg'), driver='GPKG')\n",
    "pr_soilgrid = pd.read_csv(os.path.join(Path_Output, 'pr_at_2_6m.txt'), sep='\\t').rename(columns={'pr': 'pr_soilgrid'})\n",
    "Path_Output_old = r'C:\\Users\\lli55\\Desktop\\Lingbo Li PhD\\DOC project\\Data'\n",
    "pr_hwsd = pd.read_csv(os.path.join(Path_Output_old, 'pr_at_2_6m_mase.txt'), sep='\\t').rename(columns={'pr': 'pr_hwsd'})\n",
    "\n",
    "# Merge precipitation data with geospatial data\n",
    "data = data.merge(pr_hwsd, on='COMID', how='left')\n",
    "data = data.merge(pr_soilgrid, on='COMID', how='left')\n",
    "\n",
    "# Calculate DOC-related columns\n",
    "data['doc_hwsd'] = data['soc_hwsd'] * data['pr_hwsd']\n",
    "data['doc_soilgrid'] = data['soc_soilgrid'] * data['pr_soilgrid']\n",
    "\n",
    "# update the SOC.gpkg file\n",
    "data.to_file(os.path.join(Path_Shape, 'SOC.gpkg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Step2: calculate the area weighted average Pr and DOC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def doc_reanalysis(raster, catchments):\n",
    "    catchment = catchments.copy()\n",
    "    ## Roughly find the local catchments insides the selected catchments using Sjoin\n",
    "    joined = raster.sjoin(catchment, how=\"inner\", predicate='intersects').reset_index(drop = True)\n",
    "    doc = []\n",
    "    for i in tqdm.tqdm(range(len(catchment))):\n",
    "        comids = catchment['comid'][i]\n",
    "        geometry = catchment['geometry'][i]\n",
    "        join = joined[joined['comid'] == comids].reset_index(drop = True)\n",
    "        area = [geometry.intersection(item).area for item in join['geometry']]\n",
    "        join['inter_area'] = area\n",
    "        join = join.dropna(subset = 'doc_soilgrid').reset_index(drop = True)\n",
    "        if len(join) == 0:\n",
    "            doc.append(np.nan)\n",
    "        else:\n",
    "            join['derived_doc'] = join['inter_area']*join['doc_soilgrid']\n",
    "            doc.append(np.sum(join['derived_doc'])/np.sum(join['inter_area']))\n",
    "            \n",
    "    catchment['derived_doc'] = doc\n",
    "\n",
    "    return catchment\n",
    "\n",
    "def pr_reanalysis(raster, catchments):\n",
    "    catchment = catchments.copy()\n",
    "    joined = raster.sjoin(catchment, how=\"inner\", predicate='intersects').reset_index(drop = True)\n",
    "    pr = []\n",
    "    for i in tqdm.tqdm(range(len(catchment))):\n",
    "        comids = catchment['comid'][i]\n",
    "        geometry = catchment['geometry'][i]\n",
    "        join = joined[joined['comid'] == comids].reset_index(drop = True)\n",
    "        area = [geometry.intersection(item).area for item in join['geometry']]\n",
    "        join['inter_area'] = area\n",
    "        join = join.dropna(subset = 'pr_soilgrid').reset_index(drop = True)\n",
    "        if len(join) == 0:\n",
    "            pr.append(np.nan)\n",
    "        else:\n",
    "            join['derived_pr'] = join['inter_area']*join['pr_soilgrid']\n",
    "            pr.append(np.sum(join['derived_pr'])/np.sum(join['inter_area']))\n",
    "            \n",
    "    catchment['derived_pr'] = pr\n",
    "    \n",
    "    return catchment\n",
    "\n",
    "selected = gpd.read_file(os.path.join(Path_Shape, 'SOC_compare_eval.gpkg'))\n",
    "raster = gpd.read_file(os.path.join(Path_Shape, 'SOC.gpkg'))\n",
    "data_pr = pr_reanalysis(raster, selected)\n",
    "data_doc = doc_reanalysis(raster, selected)\n",
    "data_doc['derived_pr'] = data_pr['derived_pr']\n",
    "data_doc_1 = data_doc[['comid', 'ave_doc', 'soc_soilgrid', 'derived_pr', 'derived_doc']]\n",
    "data_doc_1.to_csv(os.path.join(Path_Output, 'eval_validation.txt'), sep = '\\t', index = None)\n",
    "data_doc_1 = data_doc[['comid', 'ave_doc', 'soc_soilgrid', 'derived_pr', 'derived_doc']]\n",
    "data_doc_1.to_csv(os.path.join(Path_Output, 'eval_validation.txt'), sep = '\\t', index = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f83720f3524ad70a07f8b9522dddf168ead5e164ced993ad70b21c480e663491"
  },
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('Hyriver': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
